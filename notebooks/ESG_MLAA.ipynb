{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PHzhDjbjBdkn",
        "gk03i8XECn2P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Bag of Words**"
      ],
      "metadata": {
        "id": "8QSha0yxATBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "3-Xf64E4AZy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFn5zgo8AGNk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"preprocessed_content.csv\")\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "sns.histplot(x = \"total_score\",data = df)\n",
        "print(df[\"total_score\"].median())\n",
        "df[\"label\"] = df[\"total_score\"].apply(lambda x : \"Green\" if x < df[\"total_score\"].median() else \"Non Green\")\n",
        "df.head()\n",
        "\n",
        "x = df[\"preprocessed_content\"].values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "ntV3wMOlAnZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(x)) :\n",
        "  text = nltk.sent_tokenize(x[i])\n",
        "  processed_content = []\n",
        "  for t in text :\n",
        "    t = re.sub(\"[^a-zA-Z]\",\" \",str(t))\n",
        "    t = t.lower()\n",
        "    processed_content.append(t)\n",
        "  corpus.append(\" \".join(processed_content))\n",
        "corpus\n",
        "\n",
        "len(corpus[0])"
      ],
      "metadata": {
        "id": "05JSbdCQAsxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sent = []\n",
        "for s in corpus :\n",
        "  words = nltk.word_tokenize(s)\n",
        "  words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "  t = \" \".join(words)\n",
        "  sent.append(t)\n",
        "sent\n",
        "\n",
        "len(sent)"
      ],
      "metadata": {
        "id": "8u_h5T4tA4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "x = cv.fit_transform(sent)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,\n",
        "                                                 random_state = 42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "FOXbqcQDBCHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "base_model = MultinomialNB()\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator = base_model,\n",
        "    n_estimators = 50,\n",
        "    max_samples = 0.8,\n",
        "    bootstrap = True,\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "bag_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "ywh-4-YHBGN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdt_train = bag_model.predict(x_train)\n",
        "prdt_test = bag_model.predict(x_test)\n",
        "\n",
        "print(\"Predictions on Training Data : \\n\",prdt_train)\n",
        "print(\"\\nPredictions on Testing Data : \\n\",prdt_test)"
      ],
      "metadata": {
        "id": "KkmmekRVBKXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rob_train = bag_model.predict_proba(x_train)\n",
        "prob_test = bag_model.predict_proba(x_test)\n",
        "\n",
        "print(\"Probabilities on Training Data : \\n\",prob_train)\n",
        "print(\"\\nProbabilities on Testing Data : \\n\",prob_test)"
      ],
      "metadata": {
        "id": "2Syi5QAyBOQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc_train = accuracy_score(y_train,prdt_train)\n",
        "acc_test = accuracy_score(y_test,prdt_test)\n",
        "\n",
        "print(\"Accuracy on Training Data : \",acc_train)\n",
        "print(\"Accuracy on Testing Data : \",acc_test)"
      ],
      "metadata": {
        "id": "JNyNXns8BPNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "cf = confusion_matrix(y_test,prdt_test)\n",
        "print(\"Confusion Matrix : \\n\",cf)\n",
        "\n",
        "sns.heatmap(cf,cmap = \"coolwarm\",annot = True)\n",
        "plt.title(\"Confusion Matrix (Test Data)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qAD0gaHjBWjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "RocCurveDisplay.from_estimator(bag_model,x_test,y_test)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2J4FuT7lBWbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF - IDF**"
      ],
      "metadata": {
        "id": "PHzhDjbjBdkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "8QE5xA9KBgRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dwVmusLJBpLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "q2rGFNOsBpI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "0jUfa3zYBpGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "WYnKLJUZBpDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"preprocessed_content.csv\")"
      ],
      "metadata": {
        "id": "vfGP_fPiBpBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "w9HVwctnBo-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "mr7uLPNqBo7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "6KkKp-VpBo4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "Le_oZ99DBo1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(x = \"total_score\",data = df)"
      ],
      "metadata": {
        "id": "hhv1ThY1Boyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"total_score\"].median())"
      ],
      "metadata": {
        "id": "ReUY7OMaB1Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"] = df[\"total_score\"].apply(lambda x : \"Green\" if x < df[\"total_score\"].median() else \"Non Green\")"
      ],
      "metadata": {
        "id": "jDaC0VbBB1Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[\"preprocessed_content\"].values\n",
        "y = df[\"label\"].values"
      ],
      "metadata": {
        "id": "VAW2jlRjB1UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "OW7ZbnERB1RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(x)) :\n",
        "  text = nltk.sent_tokenize(x[i])\n",
        "  processed_content = []\n",
        "  for t in text :\n",
        "    t = re.sub(\"[^a-zA-Z]\",\" \",str(t))\n",
        "    t = t.lower()\n",
        "    processed_content.append(t)\n",
        "  corpus.append(\" \".join(processed_content))\n",
        "corpus"
      ],
      "metadata": {
        "id": "8GeiEsZgB1OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus[0])"
      ],
      "metadata": {
        "id": "MKWqDr4bB1Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sent = []\n",
        "for s in corpus :\n",
        "  words = nltk.word_tokenize(s)\n",
        "  words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "  t = \" \".join(words)\n",
        "  sent.append(t)\n",
        "sent"
      ],
      "metadata": {
        "id": "Dd6TKhj-B-BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent)"
      ],
      "metadata": {
        "id": "i0LCqiz4B9_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "MR4uicf_B98d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.fit_transform(sent)"
      ],
      "metadata": {
        "id": "xqno9Jd9B952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,\n",
        "                                                 random_state = 42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "g85bsnVnB93g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.naive_bayes import ComplementNB"
      ],
      "metadata": {
        "id": "UaQCAZGcB905"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ComplementNB()\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator = base_model,\n",
        "    n_estimators = 90,\n",
        "    max_samples = 0.8,\n",
        "    bootstrap = True,\n",
        "    random_state = 42\n",
        ")"
      ],
      "metadata": {
        "id": "5A8oE0ayB9yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "gUws-sLiB9vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prdt_train = bag_model.predict(x_train)\n",
        "prdt_test = bag_model.predict(x_test)\n",
        "\n",
        "print(\"Predictions on Training Data : \\n\",prdt_train)\n",
        "print(\"\\nPredictions on Testing Data : \\n\",prdt_test)"
      ],
      "metadata": {
        "id": "HkbweONWB9s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_train = bag_model.predict_proba(x_train)\n",
        "prob_test = bag_model.predict_proba(x_test)\n",
        "\n",
        "print(\"Probabilities on Training Data : \\n\",prob_train)\n",
        "print(\"\\nProbabilities on Testing Data : \\n\",prob_test)"
      ],
      "metadata": {
        "id": "qYL3imrbB9ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc_train = accuracy_score(y_train,prdt_train)\n",
        "acc_test = accuracy_score(y_test,prdt_test)\n",
        "\n",
        "print(\"Accuracy on Training Data : \",acc_train)\n",
        "print(\"Accuracy on Testing Data : \",acc_test)"
      ],
      "metadata": {
        "id": "uuGn4EKjB9m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "cf = confusion_matrix(y_test,prdt_test)\n",
        "print(\"Confusion Matrix : \\n\",cf)"
      ],
      "metadata": {
        "id": "MgUqyWmCCkg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cf,cmap = \"coolwarm\",annot = True)\n",
        "plt.title(\"Confusion Matrix (Test Data)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ao7nIYy2CkeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2Vec**"
      ],
      "metadata": {
        "id": "gk03i8XECn2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "qSTBADVtCkbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "1YxRHAs-CkYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "3in_xjZGCkVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "RUpWq7pACsa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"preprocessed_content.csv\")"
      ],
      "metadata": {
        "id": "0TartnjdCsYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "esblF88RCsVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "yzsqwz2eCsTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "TGklDaUqCsQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "LnmdOcVtCsNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "-lRGV0ocCsKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(x = \"total_score\", data = df)"
      ],
      "metadata": {
        "id": "T6Fm3Is0CsIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"total_score\"].median()"
      ],
      "metadata": {
        "id": "eFEOVfU1CsF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"] = df[\"total_score\"].apply(lambda x : \"Green\" if x < 29.9 else \"Non green\")"
      ],
      "metadata": {
        "id": "c9OY9yrvCsC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[\"preprocessed_content\"].values\n",
        "y = df[\"label\"].values"
      ],
      "metadata": {
        "id": "gmJVDw9_CsAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "EHOf5PyOCr9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(x)) :\n",
        "  text = nltk.sent_tokenize(x[i])\n",
        "  processed_content = []\n",
        "  for t in text :\n",
        "    t = re.sub(\"[^a-zA-Z]\",\" \",str(t))\n",
        "    t = t.lower()\n",
        "    processed_content.append(t)\n",
        "  corpus.append(\" \".join(processed_content))\n",
        "corpus"
      ],
      "metadata": {
        "id": "C7fIhaf0Cr65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "oXWcSiT9Cr4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [nltk.word_tokenize(s) for s in corpus]\n",
        "words"
      ],
      "metadata": {
        "id": "9RHFqs7sDQg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words[0])"
      ],
      "metadata": {
        "id": "7auR9OLODQeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(words)) :\n",
        "  words[i] = [lemmatizer.lemmatize(w) for w in words[i] if w not in stop_words]\n",
        "words"
      ],
      "metadata": {
        "id": "dmBJDfTaDQbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words[0])"
      ],
      "metadata": {
        "id": "G_6maSyDDQZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "nIHwWL7JDQWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(words,min_count = 2,vector_size = 300,sg = 1,\n",
        "                 window = 7)"
      ],
      "metadata": {
        "id": "kijsuzZ5DQTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.key_to_index"
      ],
      "metadata": {
        "id": "YPef2PaFDQQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vector Representation : \",model.wv[\"investor\"])\n",
        "print(\"\\nWords most similar to 'investor' : \\n\",model.wv.most_similar(\"investor\"))"
      ],
      "metadata": {
        "id": "aztVOIzLDQOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef sent_to_vector (sent,model) :\n",
        "  words = nltk.word_tokenize(sent)\n",
        "  vector = [model.wv[w] for w in words if w in model.wv]\n",
        "\n",
        "  if not vector :\n",
        "    return np.zeros(300)\n",
        "\n",
        "  return np.mean(vector,axis = 0)"
      ],
      "metadata": {
        "id": "1qEshRvyDQLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vec = np.vstack([sent_to_vector(s,model) for s in corpus])"
      ],
      "metadata": {
        "id": "MmdYqEgpDQIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_vec,y,test_size = 0.2,\n",
        "                                                 random_state = 42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "yqH3kQBzDpLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "24Y6qF-kDpI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = DecisionTreeClassifier()\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator = base_model,\n",
        "    n_estimators = 110,\n",
        "    max_samples = 0.8,\n",
        "    bootstrap = True,\n",
        "    random_state = 42\n",
        ")"
      ],
      "metadata": {
        "id": "bMFG-AX6DpGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "VBsDEpC9DpDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prdt_train = bag_model.predict(x_train)\n",
        "prdt_test = bag_model.predict(x_test)\n",
        "\n",
        "print(\"Predictions on Training Data : \\n\",prdt_train)\n",
        "print(\"\\nPredictions on Testing Data : \\n\",prdt_test)"
      ],
      "metadata": {
        "id": "hvBUi-vODpAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_train = bag_model.predict_proba(x_train)\n",
        "prob_test = bag_model.predict_proba(x_test)\n",
        "\n",
        "print(\"Probabilities on Training Data : \\n\",prob_train)\n",
        "print(\"\\nProbabilities on Testing Data : \\n\",prob_test)"
      ],
      "metadata": {
        "id": "HFKtHFN6Do-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc_train = accuracy_score(y_train,prdt_train)\n",
        "acc_test = accuracy_score(y_test,prdt_test)\n",
        "\n",
        "print(\"Accuracy on Training Data : \",acc_train)\n",
        "print(\"Accuracy on Testing Data : \",acc_test)"
      ],
      "metadata": {
        "id": "Jn9qLZ6uDwpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "cf = confusion_matrix(y_test,prdt_test)\n",
        "print(\"Confusion Matrix : \\n\",cf)"
      ],
      "metadata": {
        "id": "bYCU_kOHDwmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cf,cmap = \"coolwarm\",annot = True)\n",
        "plt.title(\"Confusion Matrix (Test Data)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kSsLFnYfDwj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}